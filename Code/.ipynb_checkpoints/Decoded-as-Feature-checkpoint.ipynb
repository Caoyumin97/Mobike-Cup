{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T06:21:19.444122Z",
     "start_time": "2020-02-25T06:21:19.439135Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geohash import decode_exactly as dec_exa_fn,decode as dec_fn\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input,Dense,Dropout,LeakyReLU as lrelu\n",
    "from tensorflow.keras.initializers import he_normal,constant\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:41:04.288127Z",
     "start_time": "2020-02-25T05:41:01.509403Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:41:04.312875Z",
     "start_time": "2020-02-25T05:41:04.308885Z"
    }
   },
   "outputs": [],
   "source": [
    "def defix(x):\n",
    "    groups = re.match(\"w(.*)\",x)\n",
    "    if groups == None:\n",
    "        print(x)\n",
    "        return -1\n",
    "    elif len(groups[1]) == 6:\n",
    "        return groups[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:41:11.385021Z",
     "start_time": "2020-02-25T05:41:04.327869Z"
    }
   },
   "outputs": [],
   "source": [
    "data['start_lon'] = data['geohashed_start_loc'].map(lambda x:dec_exa_fn(x)[0])\n",
    "data['start_lat'] = data['geohashed_start_loc'].map(lambda x:dec_exa_fn(x)[1])\n",
    "data['end_lon'] = data['geohashed_end_loc'].map(lambda x:dec_exa_fn(x)[0])\n",
    "data['end_lat'] = data['geohashed_end_loc'].map(lambda x:dec_exa_fn(x)[1])\n",
    "\n",
    "data['starttime'] = pd.to_datetime(data['starttime'])\n",
    "data['defix_start'] = data['geohashed_start_loc'].map(lambda x:defix(x))\n",
    "data['defix_end'] = data['geohashed_end_loc'].map(lambda x:defix(x))\n",
    "data.drop(['geohashed_start_loc','geohashed_end_loc'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:41:11.414888Z",
     "start_time": "2020-02-25T05:41:11.399927Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orderid</th>\n",
       "      <th>userid</th>\n",
       "      <th>bikeid</th>\n",
       "      <th>biketype</th>\n",
       "      <th>starttime</th>\n",
       "      <th>defix_start</th>\n",
       "      <th>defix_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1893973</td>\n",
       "      <td>451147</td>\n",
       "      <td>210617</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-05-14 22:16:50</td>\n",
       "      <td>x4snhx</td>\n",
       "      <td>x4snhj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4657992</td>\n",
       "      <td>1061133</td>\n",
       "      <td>465394</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-05-14 22:16:52</td>\n",
       "      <td>x4dr59</td>\n",
       "      <td>x4dquz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2965085</td>\n",
       "      <td>549189</td>\n",
       "      <td>310572</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-05-14 22:16:51</td>\n",
       "      <td>x4fgur</td>\n",
       "      <td>x4fu5n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4548579</td>\n",
       "      <td>489720</td>\n",
       "      <td>456688</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-05-14 22:16:51</td>\n",
       "      <td>x4d5r5</td>\n",
       "      <td>x4d5r4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3936364</td>\n",
       "      <td>467449</td>\n",
       "      <td>403224</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-05-14 22:16:50</td>\n",
       "      <td>x4g27p</td>\n",
       "      <td>x4g266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   orderid   userid  bikeid  biketype           starttime defix_start  \\\n",
       "0  1893973   451147  210617         2 2017-05-14 22:16:50      x4snhx   \n",
       "1  4657992  1061133  465394         1 2017-05-14 22:16:52      x4dr59   \n",
       "2  2965085   549189  310572         1 2017-05-14 22:16:51      x4fgur   \n",
       "3  4548579   489720  456688         1 2017-05-14 22:16:51      x4d5r5   \n",
       "4  3936364   467449  403224         1 2017-05-14 22:16:50      x4g27p   \n",
       "\n",
       "  defix_end  \n",
       "0    x4snhj  \n",
       "1    x4dquz  \n",
       "2    x4fu5n  \n",
       "3    x4d5r4  \n",
       "4    x4g266  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:41:14.495651Z",
     "start_time": "2020-02-25T05:41:11.427852Z"
    }
   },
   "outputs": [],
   "source": [
    "orderID = Counter(data['orderid'])\n",
    "userID = Counter(data['userid'])\n",
    "bikeID = Counter(data['bikeid'])\n",
    "bikeType = Counter(data['biketype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:41:16.388591Z",
     "start_time": "2020-02-25T05:41:16.381609Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3214096, 9, 6, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(orderID),int(len(orderID)/len(userID)),int(len(orderID)/len(bikeID)),len(bikeType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T06:04:03.680135Z",
     "start_time": "2020-02-23T06:04:00.784678Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Hashed Location Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:42:18.362763Z",
     "start_time": "2020-02-25T05:42:18.353785Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(col):\n",
    "    char_list = []\n",
    "    for ind,i in enumerate(data[col]):\n",
    "        char_list.append(list(i))\n",
    "\n",
    "    return char_list,list(np.unique(np.array(char_list)))\n",
    "\n",
    "def Vocabularizer(vocab_keys):\n",
    "    Vocab = {'token_to_idx':{},'idx_to_token':{}}\n",
    "    for ind, i in enumerate(vocab_keys):\n",
    "        Vocab['token_to_idx'][i] = ind\n",
    "        Vocab['idx_to_token'][ind] = i\n",
    "    return Vocab\n",
    "\n",
    "def Numberizer(start_seq):\n",
    "    source_encoded = np.zeros(np.array(start_seq).shape).astype(int)\n",
    "    for idx_i, seq in enumerate(start_seq):\n",
    "        for idx_j, tk in enumerate(seq):\n",
    "            source_encoded[idx_i,idx_j] = Vocab['token_to_idx'][tk]\n",
    "    return source_encoded\n",
    "\n",
    "def OneHotEncoder(vocab_keys,source_encoded):\n",
    "    onehot_token_len = len(vocab_keys)\n",
    "    onehot_vec_len = 6 * onehot_token_len\n",
    "    source_onehot_encoded = np.zeros((source_encoded.shape[0],onehot_vec_len))\n",
    "\n",
    "    for idx_i, num_seq in enumerate(source_encoded):\n",
    "        onehot_vec = np.zeros((onehot_vec_len,))\n",
    "        for idx_j, num in enumerate(num_seq):\n",
    "            onehot_vec[onehot_token_len * idx_j + num - 1] = 1\n",
    "        source_onehot_encoded[idx_i,:] = onehot_vec\n",
    "    \n",
    "    return source_onehot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:41:52.198921Z",
     "start_time": "2020-02-25T05:41:28.788315Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_seq, vocab_keys = tokenizer('defix_start')\n",
    "end_seq, _ = tokenizer('defix_end')\n",
    "\n",
    "onehot_token_len = len(vocab_keys)\n",
    "Vocab = Vocabularizer(vocab_keys)\n",
    "\n",
    "source_encoded = Numberizer(start_seq)\n",
    "target_encoded = Numberizer(end_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T05:43:00.568102Z",
     "start_time": "2020-02-25T05:42:23.609735Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "source_onehot_encoded = OneHotEncoder(vocab_keys,source_encoded)\n",
    "target_onehot_encoded = OneHotEncoder(vocab_keys,target_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Embedding by AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:22:22.493172Z",
     "start_time": "2020-02-25T07:22:22.482202Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_AE():\n",
    "    x = Input(shape=(192, ))\n",
    "    h = Dense(128, activation=lrelu(0.1), kernel_initializer=he_normal())(x)\n",
    "    h = Dense(64, activation=lrelu(0.1), kernel_initializer=he_normal())(h)\n",
    "    h = Dense(128,\n",
    "              activation=lrelu(0.1),\n",
    "              kernel_initializer=he_normal(),\n",
    "              name='dec_1')(h)\n",
    "    h = Dense(192,\n",
    "              activation=lrelu(0.1),\n",
    "              kernel_initializer=he_normal(),\n",
    "              name='dec_2')(h)\n",
    "\n",
    "    model = Model(inputs=x, outputs=h)\n",
    "    model.compile(optimizer=Adamax(lr=3e-4), loss='mse', metrics=[mae])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_embedder(model):\n",
    "    return Model(inputs = model.input,outputs = model.layers[2].output)\n",
    "\n",
    "\n",
    "def build_disembedder(model):\n",
    "    emb = Input(shape = (64,))\n",
    "    h = Dense(128,activation=lrelu(0.1),kernel_initializer=he_normal(),name='dec_1')(emb)\n",
    "    h = Dense(192,activation=lrelu(0.1),kernel_initializer=he_normal(),name='dec_2')(h)\n",
    "    DisEmbedder = Model(inputs = emb,outputs = h)\n",
    "    DisEmbedder.layers[-1].set_weights(model.layers[-1].get_weights())\n",
    "    DisEmbedder.layers[-2].set_weights(model.layers[-2].get_weights())\n",
    "    \n",
    "    return DisEmbedder\n",
    "\n",
    "\n",
    "def train_AE(source_onehot_encoded,target_onehot_encoded,batch_size,epochs):\n",
    "    model = build_AE()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.fit(source_onehot_encoded,\n",
    "                  source_onehot_encoded,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=1,\n",
    "                  validation_split=0.2,\n",
    "                  verbose=1)\n",
    "        model.fit(target_onehot_encoded,\n",
    "                  target_onehot_encoded,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=1,\n",
    "                  validation_split=0.2,\n",
    "                  verbose=1)\n",
    "    model.save('./AE.h5')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:13:51.600823Z",
     "start_time": "2020-02-25T07:07:54.352555Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2571276 samples, validate on 642820 samples\n",
      "2571276/2571276 [==============================] - 18s 7us/sample - loss: 0.0073 - mean_absolute_error: 0.0363 - val_loss: 0.0013 - val_mean_absolute_error: 0.0224\n",
      "Train on 2571276 samples, validate on 642820 samples\n",
      "2571276/2571276 [==============================] - 17s 7us/sample - loss: 7.8102e-04 - mean_absolute_error: 0.0166 - val_loss: 5.0934e-04 - val_mean_absolute_error: 0.0129\n",
      "Train on 2571276 samples, validate on 642820 samples\n",
      "2571276/2571276 [==============================] - 17s 7us/sample - loss: 4.1234e-04 - mean_absolute_error: 0.0112 - val_loss: 3.4317e-04 - val_mean_absolute_error: 0.0099\n",
      "Train on 2571276 samples, validate on 642820 samples\n",
      "2571276/2571276 [==============================] - 17s 7us/sample - loss: 3.0067e-04 - mean_absolute_error: 0.0089 - val_loss: 2.7143e-04 - val_mean_absolute_error: 0.0083\n",
      "Train on 2571276 samples, validate on 642820 samples\n",
      "2571276/2571276 [==============================] - 18s 7us/sample - loss: 2.4825e-04 - mean_absolute_error: 0.0076 - val_loss: 2.3365e-04 - val_mean_absolute_error: 0.0073\n",
      "Train on 2571276 samples, validate on 642820 samples\n",
      "2571276/2571276 [==============================] - 18s 7us/sample - loss: 2.1828e-04 - mean_absolute_error: 0.0067 - val_loss: 2.0879e-04 - val_mean_absolute_error: 0.0065\n",
      "Train on 2571276 samples, validate on 642820 samples\n",
      "2571276/2571276 [==============================] - 17s 7us/sample - loss: 1.9831e-04 - mean_absolute_error: 0.0061 - val_loss: 1.9345e-04 - val_mean_absolute_error: 0.0060\n",
      "Train on 2571276 samples, validate on 642820 samples\n",
      "2571276/2571276 [==============================] - 18s 7us/sample - loss: 1.8451e-04 - mean_absolute_error: 0.0056 - val_loss: 1.8100e-04 - val_mean_absolute_error: 0.0056\n",
      "Train on 2571276 samples, validate on 642820 samples\n",
      "2571276/2571276 [==============================] - 18s 7us/sample - loss: 1.7420e-04 - mean_absolute_error: 0.0052 - val_loss: 1.7182e-04 - val_mean_absolute_error: 0.0053\n",
      "Train on 2571276 samples, validate on 642820 samples\n",
      "2571276/2571276 [==============================] - 18s 7us/sample - loss: 1.6597e-04 - mean_absolute_error: 0.0049 - val_loss: 1.6448e-04 - val_mean_absolute_error: 0.0049\n",
      "Train on 2571276 samples, validate on 642820 samples\n",
      "2571276/2571276 [==============================] - 18s 7us/sample - loss: 1.5898e-04 - mean_absolute_error: 0.0046 - val_loss: 1.5727e-04 - val_mean_absolute_error: 0.0046\n",
      "Train on 2571276 samples, validate on 642820 samples\n",
      "2571276/2571276 [==============================] - 18s 7us/sample - loss: 1.5285e-04 - mean_absolute_error: 0.0044 - val_loss: 1.5174e-04 - val_mean_absolute_error: 0.0044\n",
      "Train on 2571276 samples, validate on 642820 samples\n",
      "2571276/2571276 [==============================] - 18s 7us/sample - loss: 1.4745e-04 - mean_absolute_error: 0.0041 - val_loss: 1.4584e-04 - val_mean_absolute_error: 0.0041\n",
      "Train on 2571276 samples, validate on 642820 samples\n",
      "2571276/2571276 [==============================] - 18s 7us/sample - loss: 1.4283e-04 - mean_absolute_error: 0.0039 - val_loss: 1.4272e-04 - val_mean_absolute_error: 0.0039\n",
      "Train on 2571276 samples, validate on 642820 samples\n",
      "2571276/2571276 [==============================] - 18s 7us/sample - loss: 1.3878e-04 - mean_absolute_error: 0.0037 - val_loss: 1.3756e-04 - val_mean_absolute_error: 0.0037\n",
      "Train on 2571276 samples, validate on 642820 samples\n",
      "2571276/2571276 [==============================] - 17s 7us/sample - loss: 1.3533e-04 - mean_absolute_error: 0.0035 - val_loss: 1.3484e-04 - val_mean_absolute_error: 0.0035\n",
      "Train on 2571276 samples, validate on 642820 samples\n",
      "2571276/2571276 [==============================] - 17s 7us/sample - loss: 1.3231e-04 - mean_absolute_error: 0.0033 - val_loss: 1.3158e-04 - val_mean_absolute_error: 0.0034\n",
      "Train on 2571276 samples, validate on 642820 samples\n",
      "2571276/2571276 [==============================] - 17s 7us/sample - loss: 1.2966e-04 - mean_absolute_error: 0.0032 - val_loss: 1.2928e-04 - val_mean_absolute_error: 0.0032\n",
      "Train on 2571276 samples, validate on 642820 samples\n",
      "2571276/2571276 [==============================] - 18s 7us/sample - loss: 1.2708e-04 - mean_absolute_error: 0.0031 - val_loss: 1.2692e-04 - val_mean_absolute_error: 0.0031\n",
      "Train on 2571276 samples, validate on 642820 samples\n",
      "2571276/2571276 [==============================] - 18s 7us/sample - loss: 1.2516e-04 - mean_absolute_error: 0.0030 - val_loss: 1.2513e-04 - val_mean_absolute_error: 0.0030\n"
     ]
    }
   ],
   "source": [
    "train_AE(source_onehot_encoded,target_onehot_encoded,batch_size=1024,epochs = 10)\n",
    "\n",
    "Embedder = build_embedder(model)\n",
    "DisEmbedder = build_disembedder(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:18:03.124070Z",
     "start_time": "2020-02-25T07:15:42.069176Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "source_embed = Embedder.predict(source_onehot_encoded)\n",
    "target_embed = Embedder.predict(target_onehot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:26:51.168090Z",
     "start_time": "2020-02-25T07:26:51.163103Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train,y_train = source_embed[:int(3e+6)],target_embed[:int(3e+6)]\n",
    "X_test,y_test = source_embed[int(3e+6):],target_embed[int(3e+6):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:40:53.059328Z",
     "start_time": "2020-02-25T07:40:53.051351Z"
    }
   },
   "outputs": [],
   "source": [
    "def characterizer(y_pred,DisEmbedder):\n",
    "    y_pred_onehot = np.abs(np.round(DisEmbedder.predict(y_pred)))\n",
    "    \n",
    "    # return to idx\n",
    "    target_encoded = np.zeros((y_pred_onehot.shape[0],6))\n",
    "    for idx_i, seq in enumerate(y_pred_onehot):\n",
    "        for idx_j in range(6):\n",
    "            onehot_vec = np.zeros((32,))\n",
    "            idx_onehot = np.argmax(seq[32 * idx_j:32 * (idx_j + 1)])\n",
    "            target_encoded[idx_i,idx_j] = idx_onehot\n",
    "    \n",
    "    # idx to geohash token\n",
    "    target_geohash = np.zeros_like(target_encoded).astype(str)\n",
    "    for idx_i, seq in enumerate(list(target_encoded)):\n",
    "        for idx_j, st in enumerate(seq):\n",
    "            target_geohash[idx_i,idx_j] = Vocab['idx_to_token'][st]\n",
    "    \n",
    "    return target_geohash\n",
    "\n",
    "\n",
    "def Acc(y_pred,y_test):\n",
    "    y_pred_char = characterizer(y_pred,DisEmbedder)\n",
    "    y_test_char = characterizer(y_test,DisEmbedder)\n",
    "    acc = (((y_test_char == y_pred_char).sum(axis = 1) - 6) >= 0).sum() / y_test.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:26:57.105220Z",
     "start_time": "2020-02-25T07:26:57.003491Z"
    }
   },
   "outputs": [],
   "source": [
    "x = Input(shape = (64,))\n",
    "h = Dense(128,activation='relu',kernel_initializer=he_normal())(x)\n",
    "h = Dense(64,activation='relu',kernel_initializer=he_normal())(h)\n",
    "h = Dense(128,activation='relu',kernel_initializer=he_normal())(h)\n",
    "h = Dense(64,activation='relu',kernel_initializer=he_normal())(h)\n",
    "\n",
    "model = Model(inputs = x,outputs = h)\n",
    "model.compile(optimizer = Adamx(3e-4),loss = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T07:43:29.409332Z",
     "start_time": "2020-02-25T07:41:58.084487Z"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "    hist = model.fit(X_train,y_train,batch_size=1024,epochs=1,verbose=1)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.round(y_pred).astype(int)\n",
    "    print('Epoch: %d, Accuracy: %.2f%%' %(epoch + 1,100 * Acc(y_pred,y_test)))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
